shading = "lift", col = "white", limit = 100)#, limit = 100)
plot(rules, limit = 100)
inspect(sort(rules, by = "lift"))
inspect(head(sort(rules, by = "lift")),3)
inspect(head(sort(rules, by = "lift"),3))
inspect(head(sort(rules, by = "confidence"),3))
highLiftRules <- head(sort(rules, by="lift"), 5)
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
highLiftRules <- head(sort(rules, by="confidence"), 5)
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
plot(highLiftRules, method = "graph")
highLiftRules <- head(sort(rules, by="confidence"), 5)
plot(highLiftRules, method = "graph")
highLiftRules <- head(sort(rules, by="lift"), 5)
plot(highLiftRules, method = "graph")
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
apply(Groceries@data[,1:100], 2,
function(r) paste(Groceries@itemInfo[r,"labels"],
collapse=", "))
apply(Groceries@data[,1:2], 2,
function(r) paste(Groceries@itemInfo[r,"labels"],
collapse=", "))
summary(itemsets.1)
inspect(sort(itemsets.3, by = "support"))
# plot rules
plot(rules)
plot(rules, measure = c("support", "confidence"),
shading = "lift", col = "black")#, limit = 100)
# View top rules sorted by confidence, lift
inspect(head(sort(rules, by = "lift"),3))
inspect(head(sort(rules, by = "leverage"),3))
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
plot(highLiftRules, method = "graph")
highLiftRules <- head(sort(rules, by="confidence"), 5)
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
# Graph rules
highLiftRules <- head(sort(rules, by="lift"), 5)
plot(highLiftRules, method = "graph")
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
highConfidenceRules = head(sort(rules, by="lift"), 5)
plot(highConfidenceRules, method = "graph")
plot(highConfidenceRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
# Graph rules
highLiftRules <- head(sort(rules, by="lift"), 5)
plot(highLiftRules, method = "graph")
plot(highLiftRules, method = "graph", engine = "igraph",
edgeCol = "blue", alpha = 1)
source("C:/Users/Siah Jin Thau/OneDrive/Desktop/Notes/Introduction to Data Science - DSA1101/Finals Past Paper/Finals Practice Paper.R", echo=TRUE)
# Plot how FPR and TPR changes as the threshold changes
library(ROCR)
pred = prediction(M2$fitted.values , data$Status)
roc = performance(pred , "tpr", "fpr")
alpha = round (as.numeric(unlist(roc@alpha.values)) ,4)
fpr = round(as.numeric(unlist(roc@x.values)) ,4)
tpr = round(as.numeric(unlist(roc@y.values)) ,4)
plotData2 = data.frame(
alpha = alpha,
fpr = fpr,
tpr = tpr
)
library(tidyverse)
plotData2 %>%
gather(key = "variable", value = "value", -alpha) %>%
ggplot(aes(x = alpha, y = value)) +
geom_line(aes(color = variable)) +
coord_equal()
data = read.csv("Data/hdbresale_reg.csv")
glimpse(data)
library(dplyr)
glimpse(data)
table(data$town)
table(data$street_name)
table(data$flat_model)
table(data$flat_type)
data %>%
arrange(flat_type)
data %>%
arrange(flat_type) %>%
select(flat_type)
table(data$flat_type)
help(sample)
sample(c(1,2,3,4,5))
sample(c(1,2,3,4,5), 3)
sample(c(1,2,3,4,5), 3)
data %>%
arrange(flat_type) %>%
table()
arrangedData = data %>%
arrange(flat_type)
arrangedData %>%
select(flat_type) %>%
table()
data1 = read.csv("Data/data1-finals.csv")
data2 = read.csv("Data/data2-finals.csv")
head(data1)
head(data2)
length(data1$Patient)
length(unique(data1$Patient))
# Load packages
library(dplyr)
# Set seed according to instructions
set.seed(2811)
# Load dataset
data1 = read.csv("Data/data1-finals.csv")
head(data1)
# Form logistic regression model
data1 = data1 %>%
mutate(across(c(Y,`T`), as.factor))
glimpse(data1)
# Form logistic regression model
M1 = data1 %>%
mutate(across(c(Y,`T`), as.factor)) %>%
select(!Patient)
M1
glimpse(M1)
# Load dataset
data1 = read.csv("Data/data1-finals.csv")
# Form logistic regression model
data1 = data1 %>%
mutate(across(c(Y,`T`), as.factor))
M1 = glm(Y ~ `T` + Duration, data1, family = binomial)
# Report insignificant regressors
summary(M1)
# Report insignificant regressors
summary(M1)$coefficient
M1 = glm(Y ~ `T` + Duration + 0, data1, family = binomial)
# Report insignificant regressors
summary(M1)$coefficient
M1 = glm(Y ~ `T` + Duration, data1, family = binomial)
# Report insignificant regressors
summary(M1)$coefficient
exp(0.0682)
exp(-1.666)
exp(1.666)
library(pROC)
predict(M1, type = "response")
predictions = predict(M1, type = "response")
rocobj1 = roc(data1$Y, predictions)
auc1 = round(auc(rocobj1),4)
ggroc(list(M1 = rocobj1)) +
labs(title = "ROC Curve",
subtitle = paste0('AUC = ', auc1)) +
geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
color="darkgrey", linetype="dashed") +
coord_equal()
library(ggplot2)
ggroc(list(M1 = rocobj1)) +
labs(title = "ROC Curve",
subtitle = paste0('AUC = ', auc1)) +
geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
color="darkgrey", linetype="dashed") +
coord_equal()
library(ROCR)
# Plot TPR and FPR vs
pred = prediction(predictions , data1$Y)
roc = performance(pred , "tpr", "fpr")
alpha = round (as.numeric(unlist(roc@alpha.values)) ,4)
fpr = round(as.numeric(unlist(roc@x.values)) ,4)
tpr = round(as.numeric(unlist(roc@y.values)) ,4)
plotData2 = data.frame(
alpha = alpha,
fpr = fpr,
tpr = tpr
)
library(tidyverse)
plotData2 %>%
gather(key = "variable", value = "value", -alpha) %>%
ggplot(aes(x = alpha, y = value)) +
geom_line(aes(color = variable)) +
coord_equal()
# Find best threshold value given FPR <= 0.5
predictions
# Find best threshold value given FPR <= 0.5
predictions < 0.25
# Find best threshold value given FPR <= 0.5
predictions[which(predictions < 0.25)]
# Find best threshold value given FPR <= 0.5
arrange(predictions[which(predictions < 0.25)])
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[-1]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[-2]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[1]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[0]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[-1]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[7]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[8]
# Predict for patient A and patient B.
newdata = tibble(
D = c(80, 125),
`T` = as.factor(c(0,1))
)
data2 = read.csv("Data/data2-finals.csv")
predict(M1, newdata)
# Predict for patient A and patient B.
newdata = tibble(
Duration = c(80, 125),
`T` = as.factor(c(0,1))
)
predict(M1, newdata)
predict(M1, newdata, "class")
predict(M1, newdata, "response")
predict(M1, newdata, "link")
predict(M1, newdata, "terms")
predict(M1, newdata, "link")
predict(M1, newdata, "response")
library(e1071)
glimpse(data1)
# Build Model
M2 = naiveBayes(Y ~ `T` + Duration, data1)
predict(M2, type = "class")
predict(M2, data1, type = "class")
data1$Y
predict(M2, data1, type = "class") == data1$Y
sum(predict(M2, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M2, newdata, type = "raw")
library(rpart)
library(rpart.plot)
# Build decision tree model
M3 = rpart(formula = formula,
method = "class",
data = df,
control = rpart.control(minsplit = 4),
parms = list(split = 'information'))
# Build decision tree model
M3 = rpart(formula = Y ~ `T` + Duration,
method = "class",
data = data1,
control = rpart.control(minsplit = 4),
parms = list(split = 'information'))
# Identify most important feature
rpart.plot(M3,
type = 4,
extra = 2,
clip.right.labs = FALSE,
varlen = 0,
faclen = 0)
predict(M3, data1, type = "class")
# Most important feature is Duration
sum(predict(M3, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M3, newdata, type = "prob")
newdata
# Predict probability using patient data from question 8
predict(M3, newdata, type = "class")
# Load dataset
data2 = read.csv("Data/data2-finals.csv")
# Load dataset
data2 = read.csv("Data/data2-finals.csv")
data2
# Perform K-means clustering and obtain WSS for each respective k value
wss = numeric(10)
for (k in 1:10) {
kout = data2 %>%
kmeans(centers = k)
wss[k] = kout$tot.withinss
}
k
wss
plotData = data.frame(k = c(1:10),
wss = wss)
plotData = data.frame(k = c(1:10),
wss = wss); plotData
# Plot WSS against k
ggplot(plotData, aes(k, wss)) +
geom_point() +
geom_line() +
labs(
x = "Number of Clusters",
y = "Within Sum of Squares"
)
kout = data2 %>%
kmeans(centers = 3)
kout
kout$centers
kout$size
# Load packages
library(dplyr)
library(pROC)
library(ggplot2)
library(ROCR)
library(tidyverse)
library(e1071)
library(rpart)
library(rpart.plot)
# Set seed according to instructions
set.seed(2811)
# Load dataset
data1 = read.csv("Data/data1-finals.csv")
# Form logistic regression model
data1 = data1 %>%
mutate(across(c(Y,`T`), as.factor))
M1 = glm(Y ~ `T` + Duration, data1, family = binomial)
summary(M1)
# Report insignificant regressors
summary(M1)$coefficient
exp(0.06820776)
# Plot ROC and derive AUC
predictions = predict(M1, type = "response")
rocobj1 = roc(data1$Y, predictions)
auc1 = round(auc(rocobj1),4)
ggroc(list(M1 = rocobj1)) +
labs(title = "ROC Curve",
subtitle = paste0('AUC = ', auc1)) +
geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
color="darkgrey", linetype="dashed") +
coord_equal()
# Plot TPR and FPR vs threshold, alpha
pred = prediction(predictions , data1$Y)
roc = performance(pred , "tpr", "fpr")
alpha = round (as.numeric(unlist(roc@alpha.values)) ,4)
fpr = round(as.numeric(unlist(roc@x.values)) ,4)
tpr = round(as.numeric(unlist(roc@y.values)) ,4)
plotData2 = data.frame(
alpha = alpha,
fpr = fpr,
tpr = tpr
)
plotData2 %>%
gather(key = "variable", value = "value", -alpha) %>%
ggplot(aes(x = alpha, y = value)) +
geom_line(aes(color = variable)) +
coord_equal()
# Find best threshold value given FPR <= 0.5
# Based on the graph, we can see that for alpha less than 0.25,
# the value of fpr stays constant then spikes.
# Doing some calculation, we can check that the alpha value
# where the FPR spikes is 0.2039.
predictions
# Find best threshold value given FPR <= 0.5
# Based on the graph, we can see that for alpha less than 0.25,
# the value of fpr stays constant then spikes.
# Doing some calculation, we can check that the alpha value
# where the FPR spikes is 0.2039.
sort(predictions)
sort(predictions[which(predictions < 0.25)])[8]
sort(predictions[which(predictions < 0.25)])
sort(predictions[which(predictions < 0.25)], desc = T)
# Find best threshold value given FPR <= 0.5
# Based on the graph, we can see that for alpha less than 0.25,
# the value of fpr stays constant then spikes.
# We can check that the alpha value where the FPR spikes is 0.2039.
sort(predictions)
# Predict for patient A and patient B.
newdata = tibble(
Duration = c(80, 125),
`T` = as.factor(c(0,1))
)
predict(M1, newdata, "response")
# Build naive bayes model
M2 = naiveBayes(Y ~ `T` + Duration, data1)
# Calculate accuracy
sum(predict(M2, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M2, newdata, type = "raw")
# Build decision tree model
M3 = rpart(formula = Y ~ `T` + Duration,
method = "class",
data = data1,
control = rpart.control(minsplit = 4),
parms = list(split = 'information'))
# Identify most important feature
rpart.plot(M3,
type = 4,
extra = 2,
clip.right.labs = FALSE,
varlen = 0,
faclen = 0)
# Calculate accuracy of decision tree
sum(predict(M3, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M3, newdata, type = "class")
# Load dataset
data2 = read.csv("Data/data2-finals.csv")
# Perform K-means clustering and obtain WSS for each respective k value
wss = numeric(10)
for (k in 1:10) {
kout = data2 %>%
kmeans(centers = k)
wss[k] = kout$tot.withinss
}
plotData = data.frame(k = c(1:10),
wss = wss); plotData
# Plot WSS against k and choose k
ggplot(plotData, aes(k, wss)) +
geom_point() +
geom_line() +
labs(
x = "Number of Clusters",
y = "Within Sum of Squares"
)
# Report centroids and sizes of each cluster.
kout = data2 %>%
kmeans(centers = 3)
kout$centers
kout$size
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.02) +
scale_y_continuous(trans = scales::pseudo_log_trans(sigma = 0.01)) +
scale_x_continuous(trans = scales::pseudo_log_trans(sigma = 0.01))
library(dplyr)
library(ggplot2)
setwd("C:/Users/Siah Jin Thau/OneDrive/Desktop/Documents/Professional/Projects/Youtube Trending Video Analysis Project")
source('Cleaning Youtube Data.R')
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.02) +
scale_y_continuous(trans = scales::pseudo_log_trans(sigma = 0.01)) +
scale_x_continuous(trans = scales::pseudo_log_trans(sigma = 0.01))
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.02) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.01) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.001) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.005) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.01) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.1) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.05) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
mutate(DatesDiff = trending_Date - published_Date) %>%
ggplot(aes(x = as.numeric(DatesDiff))) +
geom_bar() +
xlab("Days until trending") +
ylab("Count") +
facet_wrap(~ categoryName,
ncol = 3,
scales = "free_y")
NA_youtube_trending_data %>%
mutate(DatesDiff = trending_Date - published_Date) %>%
ggplot(aes(x = as.numeric(DatesDiff))) +
geom_bar() +
ggtitle("Distribution of Days until Trending, faceted by category")
NA_youtube_trending_data %>%
mutate(DatesDiff = trending_Date - published_Date) %>%
ggplot(aes(x = as.numeric(DatesDiff))) +
geom_bar() +
ggtitle("Distribution of Days until Trending, faceted by category") +
xlab("Days until trending") +
ylab("Count") +
facet_wrap(~ categoryName,
ncol = 3,
scales = "free_y")
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.05) +
ggtitle("Scatterplot of View Count vs Likes") +
xlab("Likes") +
ylab("View Count") +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, dislikes)) +
geom_point()
NA_youtube_trending_data %>%
ggplot(aes(likes, comment_count)) +
geom_point()
NA_youtube_trending_data %>%
ggplot(aes(dislikes, comment_count)) +
geom_point()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point()
