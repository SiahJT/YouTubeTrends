tpr = tpr
)
library(tidyverse)
plotData2 %>%
gather(key = "variable", value = "value", -alpha) %>%
ggplot(aes(x = alpha, y = value)) +
geom_line(aes(color = variable)) +
coord_equal()
# Find best threshold value given FPR <= 0.5
predictions
# Find best threshold value given FPR <= 0.5
predictions < 0.25
# Find best threshold value given FPR <= 0.5
predictions[which(predictions < 0.25)]
# Find best threshold value given FPR <= 0.5
arrange(predictions[which(predictions < 0.25)])
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[-1]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[-2]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[1]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[0]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[-1]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[7]
# Find best threshold value given FPR <= 0.5
sort(predictions[which(predictions < 0.25)])[8]
# Predict for patient A and patient B.
newdata = tibble(
D = c(80, 125),
`T` = as.factor(c(0,1))
)
data2 = read.csv("Data/data2-finals.csv")
predict(M1, newdata)
# Predict for patient A and patient B.
newdata = tibble(
Duration = c(80, 125),
`T` = as.factor(c(0,1))
)
predict(M1, newdata)
predict(M1, newdata, "class")
predict(M1, newdata, "response")
predict(M1, newdata, "link")
predict(M1, newdata, "terms")
predict(M1, newdata, "link")
predict(M1, newdata, "response")
library(e1071)
glimpse(data1)
# Build Model
M2 = naiveBayes(Y ~ `T` + Duration, data1)
predict(M2, type = "class")
predict(M2, data1, type = "class")
data1$Y
predict(M2, data1, type = "class") == data1$Y
sum(predict(M2, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M2, newdata, type = "raw")
library(rpart)
library(rpart.plot)
# Build decision tree model
M3 = rpart(formula = formula,
method = "class",
data = df,
control = rpart.control(minsplit = 4),
parms = list(split = 'information'))
# Build decision tree model
M3 = rpart(formula = Y ~ `T` + Duration,
method = "class",
data = data1,
control = rpart.control(minsplit = 4),
parms = list(split = 'information'))
# Identify most important feature
rpart.plot(M3,
type = 4,
extra = 2,
clip.right.labs = FALSE,
varlen = 0,
faclen = 0)
predict(M3, data1, type = "class")
# Most important feature is Duration
sum(predict(M3, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M3, newdata, type = "prob")
newdata
# Predict probability using patient data from question 8
predict(M3, newdata, type = "class")
# Load dataset
data2 = read.csv("Data/data2-finals.csv")
# Load dataset
data2 = read.csv("Data/data2-finals.csv")
data2
# Perform K-means clustering and obtain WSS for each respective k value
wss = numeric(10)
for (k in 1:10) {
kout = data2 %>%
kmeans(centers = k)
wss[k] = kout$tot.withinss
}
k
wss
plotData = data.frame(k = c(1:10),
wss = wss)
plotData = data.frame(k = c(1:10),
wss = wss); plotData
# Plot WSS against k
ggplot(plotData, aes(k, wss)) +
geom_point() +
geom_line() +
labs(
x = "Number of Clusters",
y = "Within Sum of Squares"
)
kout = data2 %>%
kmeans(centers = 3)
kout
kout$centers
kout$size
# Load packages
library(dplyr)
library(pROC)
library(ggplot2)
library(ROCR)
library(tidyverse)
library(e1071)
library(rpart)
library(rpart.plot)
# Set seed according to instructions
set.seed(2811)
# Load dataset
data1 = read.csv("Data/data1-finals.csv")
# Form logistic regression model
data1 = data1 %>%
mutate(across(c(Y,`T`), as.factor))
M1 = glm(Y ~ `T` + Duration, data1, family = binomial)
summary(M1)
# Report insignificant regressors
summary(M1)$coefficient
exp(0.06820776)
# Plot ROC and derive AUC
predictions = predict(M1, type = "response")
rocobj1 = roc(data1$Y, predictions)
auc1 = round(auc(rocobj1),4)
ggroc(list(M1 = rocobj1)) +
labs(title = "ROC Curve",
subtitle = paste0('AUC = ', auc1)) +
geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
color="darkgrey", linetype="dashed") +
coord_equal()
# Plot TPR and FPR vs threshold, alpha
pred = prediction(predictions , data1$Y)
roc = performance(pred , "tpr", "fpr")
alpha = round (as.numeric(unlist(roc@alpha.values)) ,4)
fpr = round(as.numeric(unlist(roc@x.values)) ,4)
tpr = round(as.numeric(unlist(roc@y.values)) ,4)
plotData2 = data.frame(
alpha = alpha,
fpr = fpr,
tpr = tpr
)
plotData2 %>%
gather(key = "variable", value = "value", -alpha) %>%
ggplot(aes(x = alpha, y = value)) +
geom_line(aes(color = variable)) +
coord_equal()
# Find best threshold value given FPR <= 0.5
# Based on the graph, we can see that for alpha less than 0.25,
# the value of fpr stays constant then spikes.
# Doing some calculation, we can check that the alpha value
# where the FPR spikes is 0.2039.
predictions
# Find best threshold value given FPR <= 0.5
# Based on the graph, we can see that for alpha less than 0.25,
# the value of fpr stays constant then spikes.
# Doing some calculation, we can check that the alpha value
# where the FPR spikes is 0.2039.
sort(predictions)
sort(predictions[which(predictions < 0.25)])[8]
sort(predictions[which(predictions < 0.25)])
sort(predictions[which(predictions < 0.25)], desc = T)
# Find best threshold value given FPR <= 0.5
# Based on the graph, we can see that for alpha less than 0.25,
# the value of fpr stays constant then spikes.
# We can check that the alpha value where the FPR spikes is 0.2039.
sort(predictions)
# Predict for patient A and patient B.
newdata = tibble(
Duration = c(80, 125),
`T` = as.factor(c(0,1))
)
predict(M1, newdata, "response")
# Build naive bayes model
M2 = naiveBayes(Y ~ `T` + Duration, data1)
# Calculate accuracy
sum(predict(M2, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M2, newdata, type = "raw")
# Build decision tree model
M3 = rpart(formula = Y ~ `T` + Duration,
method = "class",
data = data1,
control = rpart.control(minsplit = 4),
parms = list(split = 'information'))
# Identify most important feature
rpart.plot(M3,
type = 4,
extra = 2,
clip.right.labs = FALSE,
varlen = 0,
faclen = 0)
# Calculate accuracy of decision tree
sum(predict(M3, data1, type = "class") == data1$Y)/length(data1$Y)
# Predict probability using patient data from question 8
predict(M3, newdata, type = "class")
# Load dataset
data2 = read.csv("Data/data2-finals.csv")
# Perform K-means clustering and obtain WSS for each respective k value
wss = numeric(10)
for (k in 1:10) {
kout = data2 %>%
kmeans(centers = k)
wss[k] = kout$tot.withinss
}
plotData = data.frame(k = c(1:10),
wss = wss); plotData
# Plot WSS against k and choose k
ggplot(plotData, aes(k, wss)) +
geom_point() +
geom_line() +
labs(
x = "Number of Clusters",
y = "Within Sum of Squares"
)
# Report centroids and sizes of each cluster.
kout = data2 %>%
kmeans(centers = 3)
kout$centers
kout$size
setwd("C:/Users/Siah Jin Thau/OneDrive/Desktop/Documents/Professional/Projects/Youtube Trending Video Analysis Project")
# Set working directory
setwd("C:/Users/Siah Jin Thau/OneDrive/Desktop/Documents/Professional/Projects/Youtube Trending Video Analysis Project")
# Import packages
library(dplyr)
# Import data from prev file
source('Importing Youtube Data.R')
# Check for NA/NULL values
sum(is.na(NA_youtube_trending_data))
# Remove variables that will not be used in the analysis
# Video ID, Channel ID, Category ID are each unique random values
# Analyzing the thumbnails would be cool, but that will be saved
# for another, more visual heavy analysis
NA_youtube_trending_data = NA_youtube_trending_data %>%
select(!c("video_id",
"channelId",
"categoryId",
"thumbnail_link"))
# Convert date column data type from string to DateTime
NA_youtube_trending_data$published_Date = as.Date(
NA_youtube_trending_data$publishedAt,
format='%Y-%m-%d')
NA_youtube_trending_data$trending_Date = as.Date(
NA_youtube_trending_data$trending_date,
format='%Y-%m-%d')
NA_youtube_trending_data = NA_youtube_trending_data %>%
select(!c("publishedAt", "trending_date"))
# Separate tags from one large string into a list, with
# each tag representing an element in the list
NA_youtube_trending_data$tags =
as.list(
strsplit(NA_youtube_trending_data$tags, split = "|", fixed=TRUE))
# Convert string data types to factors as needed
NA_youtube_trending_data = NA_youtube_trending_data %>%
mutate(
across(c(channelTitle,
comments_disabled,
ratings_disabled,
categoryName),
as.factor)
)
glimpse(NA_youtube_trending_data)
# Standardise column names
names(NA_youtube_trending_data) =
c("title", "channel_title", "tags", "view_count", "likes",
"dislikes", "comment_count", "comments_disabled",
"ratings_disabled", "description", "category_name",
"published_date", "trending_date")
glimpse(NA_youtube_trending_data)
# Convert to appropriate data types as needed
NA_youtube_trending_data = NA_youtube_trending_data %>%
mutate(
across(c(channelTitle,
categoryName),
as.factor)
) %>%
mutate(
across(c(comments_disabled,
ratings_disabled),
as.logical)
)
# Import data from prev file
source('Importing Youtube Data.R')
# Check for NA/NULL values
sum(is.na(NA_youtube_trending_data))
# Remove variables that will not be used in the analysis
# Video ID, Channel ID, Category ID are each unique random values
# Analyzing the thumbnails would be cool, but that will be saved
# for another, more visual heavy analysis
NA_youtube_trending_data = NA_youtube_trending_data %>%
select(!c("video_id",
"channelId",
"categoryId",
"thumbnail_link"))
# Convert date column data type from string to DateTime
NA_youtube_trending_data$published_Date = as.Date(
NA_youtube_trending_data$publishedAt,
format='%Y-%m-%d')
NA_youtube_trending_data$trending_Date = as.Date(
NA_youtube_trending_data$trending_date,
format='%Y-%m-%d')
NA_youtube_trending_data = NA_youtube_trending_data %>%
select(!c("publishedAt", "trending_date"))
# Separate tags from one large string into a list, with
# each tag representing an element in the list
NA_youtube_trending_data$tags =
as.list(
strsplit(NA_youtube_trending_data$tags, split = "|", fixed=TRUE))
# Convert to appropriate data types as needed
NA_youtube_trending_data = NA_youtube_trending_data %>%
mutate(
across(c(channelTitle,
categoryName),
as.factor)
) %>%
mutate(
across(c(comments_disabled,
ratings_disabled),
as.logical)
)
# Standardize column names
names(NA_youtube_trending_data) =
c("title", "channel_title", "tags", "view_count", "likes",
"dislikes", "comment_count", "comments_disabled",
"ratings_disabled", "description", "category_name",
"published_date", "trending_date")
# Glimpse dataset
glimpse(NA_youtube_trending_data)
NA_youtube_trending_data$comments_disabled
sum(NA_youtube_trending_data$comments_disabled)
class(NA_youtube_trending_data$comments_disabled)
class(TRUE)
# Glimpse dataset
glimpse(NA_youtube_trending_data)
setwd("C:/Users/Siah Jin Thau/OneDrive/Desktop/Documents/Professional/Projects/Youtube Trending Video Analysis Project")
library(dplyr)
library(ggplot2)
source('Cleaning Youtube Data.R')
NA_youtube_trending_data %>%
group_by(category_name) %>%
summarise(Count = n()) %>%
ggplot(aes(x = reorder(category_name, Count), y = Count)) +
geom_bar(stat = "identity", fill = "white", color = "black") +
geom_hline(yintercept = as.numeric(
NA_youtube_trending_data %>%
group_by(category_name) %>%
summarise(Count = n()) %>%
summarise(Avg = mean(Count))
), linetype = 2) +
ggtitle("Most popular video categories") +
xlab("Category Name") +
ylab("Number of videos") +
coord_flip()
print(NA_youtube_trending_data %>%
group_by(channel_title) %>%
summarise(Count = n()) %>%
arrange(desc(Count)), n = 25)
NA_youtube_trending_data %>%
mutate(DatesDiff = trending_date - published_date) %>%
ggplot(aes(x = as.numeric(DatesDiff))) +
geom_bar(fill = "white", color = "black") +
ggtitle("Distribution of Days until Trending, faceted by category") +
xlab("Days until trending") +
ylab("Count") +
facet_wrap(~ category_name,
ncol = 3,
scales = "free_y")
tagsList = unlist(NA_youtube_trending_data$tags)
tagsFrequency = as.data.frame(table(tagsList))
tagsFrequency %>%
arrange(desc(Freq)) %>%
head()
NA_youtube_trending_data %>%
ggplot(aes(likes, view_count)) +
geom_point(alpha = 0.05) +
geom_smooth(se = TRUE) +
ggtitle("Scatterplot of View Count vs Likes") +
xlab("Likes") +
ylab("View Count") +
scale_y_continuous(trans=scales::pseudo_log_trans(base = 10)) +
scale_x_continuous(trans=scales::pseudo_log_trans(base = 10))
# Mean of log(view_count)
meanValvc = mean(NA_youtube_trending_data %>%
filter(view_count != 0) %>%
pull(view_count) %>%
log(base = 10))
# Standard deviation of log(view_count)
sdValvc = sd(NA_youtube_trending_data %>%
filter(view_count != 0) %>%
pull(view_count) %>%
log(base = 10))
# Plotting density histogram of view count
NA_youtube_trending_data %>%
filter(view_count != 0) %>%
mutate(log_view_count = log(view_count, base = 10)) %>%
ggplot(aes(x = log_view_count)) +
geom_histogram(aes(y = ..density..), bins = 100,
fill = "white", color = "black") +
stat_function(fun = dnorm,
args = list(mean = meanValvc,
sd = sdValvc),
color = "red") +
labs(title = "Density histogram of log(View Count)",
subtitle = "With overlaid normal distribution",
x = "log(View Count)", y = "Density")
# Mean of log(likes)
meanVall = mean(NA_youtube_trending_data %>%
filter(likes != 0) %>%
pull(likes) %>%
log(base = 10))
# Standard deviation of log(likes)
sdVall = sd(NA_youtube_trending_data %>%
filter(likes != 0) %>%
pull(likes) %>%
log(base = 10))
# Plotting density histogram of likes
NA_youtube_trending_data %>%
filter(likes != 0) %>%
mutate(log_likes = log(likes, base = 10)) %>%
ggplot(aes(x = log_likes)) +
geom_histogram(aes(y = ..density..), bins = 100,
fill = "white", color = "black") +
stat_function(fun = dnorm,
args = list(mean = meanVall,
sd = sdVall),
color = "red") +
labs(title = "Density histogram of log(Likes)",
subtitle = "With overlaid normal distribution",
x = "log(View Count)", y = "Density")
NA_youtube_trending_data %>%
filter(view_count != 0 & likes != 0 )%>%
mutate(logViewCount = log(view_count),
logLikes = log(likes)) %>%
ggplot(aes(logLikes, logViewCount)) +
geom_point(alpha = 0.05) +
facet_wrap(~ categoryName, scales = "free") +
geom_quantile(quantiles = c(0.1, 0.5, 0.9))
NA_youtube_trending_data %>%
filter(view_count != 0 & likes != 0 )%>%
mutate(logViewCount = log(view_count),
logLikes = log(likes)) %>%
ggplot(aes(logLikes, logViewCount)) +
geom_point(alpha = 0.05) +
facet_wrap(~ category_name, scales = "free") +
geom_quantile(quantiles = c(0.1, 0.5, 0.9))
NA_youtube_trending_data %>%
ggplot(aes(x = likes)) +
geom_histogram(bins = 100) +
scale_x_log10()
# Plotting density histogram of likes
NA_youtube_trending_data %>%
filter(likes != 0) %>%
mutate(log_likes = log(likes, base = 10)) %>%
ggplot(aes(x = log_likes)) +
geom_histogram(aes(y = ..density..), bins = 100,
fill = "white", color = "black") +
stat_function(fun = dnorm,
args = list(mean = meanVall,
sd = sdVall),
color = "red") +
labs(title = "Density histogram of log(Likes)",
subtitle = "With overlaid normal distribution",
x = "log(View Count)", y = "Density")
NA_youtube_trending_data %>%
ggplot(aes(dislikes, view_count)) +
geom_point(alpha = 0.05) +
ggtitle("Scatterplot of View Count vs Likes") +
xlab("Likes") +
ylab("View Count") +
scale_y_log10() +
scale_x_log10()
rm(meanValvc, sdValvc)
rm(meanVall, sdVall)
rm(tagsList)
NA_youtube_trending_data %>%
ggplot(aes(likes, dislikes)) +
geom_point(alpha = 0.05) +
geom_smooth(method = 'lm', fullrange = TRUE) +
facet_wrap(~ categoryName) +
scale_y_log10() +
scale_x_log10()
NA_youtube_trending_data %>%
ggplot(aes(likes, dislikes)) +
geom_point(alpha = 0.05) +
geom_smooth(method = 'lm', fullrange = TRUE) +
facet_wrap(~ category_name) +
scale_y_log10() +
scale_x_log10()
